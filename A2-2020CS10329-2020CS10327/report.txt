Atul Jeph - 2020CS10329        Aryan Gaurav - 2020CS10327
For part-A, where our agent would play against the random agent, we implemented an expectimax function named dfs_expectimax. The core idea of our implementation is that here, our function tries to maximize the difference between our agent’s score and the opponent’s score. The next move, both in expectimax and alpha-beta pruning, would be either a standard move or a popOut move, depending upon where the difference in scores gets maximized, with our score being higher. We have implemented depth-limited search with depth varying according to the column width (inversely proportional). The leaves of the search tree are defined based on two factors, either the search gets performed till the specified depth within the time limit, or it reaches the endgame state.
We have also created a move_state function which takes in the current state, creates a copy, performs the specified action on it, and returns the modified state. We have used it in the expectimax function to get the next states.
For Part-B, our AI agent would be playing against an adversary, either human or another AI agent. Since here, we are assuming our opponent to be an intelligent agent (unlike the random agent in part-A), we can implement minimax algorithm instead of expectimax. But, we have made further improvement by implementing alpha-beta pruning. Here, as the name suggests, nodes which would not be affecting the values of the parent nodes, were pruned away instead of unnecessary expansion. Here, we experimented on time limit and column width to set the appropriate depths for depth-limited search. The depths came out to be in direct proportion with the time limit and inverse with column width. We also used the move_state function for getting the next states.
Statistics::
In part-A, we ran our implementation on the four test cases provided and the results were as follows:: our agent’s score was nearly 3 times the opponent’s score in case-1, nearly 2.5 times in case-2 and 4, and 2 times in case-3. We also tried out custom cases like 12*12 with popOut=10; our agent’s score was nearly 2.6 times the opponent’s score.
In part-B, when the agent ran against itself, the score of both the players were nearly the same. 
And when we tried to play against our implementation, it scored higher than us. 

Note:: We also saw some wastage of popOut moves when the dimensions of the board were low. At times, it kept popping out and filling in, from the same position, until the popOut moves got exhausted. However, we didn’t face this problem, when dealing in higher dimensions. 
